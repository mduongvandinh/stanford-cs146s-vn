# ğŸ§  CONTEXT ROT
## Understanding Degradation in AI Context Windows

---

# ğŸ“– Má»¤C Lá»¤C

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [Core Problem](#2-core-problem)
3. [NguyÃªn nhÃ¢n & Patterns](#3-nguyÃªn-nhÃ¢n--patterns)
4. [Effects on Context Windows](#4-effects-on-context-windows)
5. [Context Engineering Solutions](#5-context-engineering-solutions)
6. [Tá»« Ä‘iá»ƒn Keywords](#6-tá»«-Ä‘iá»ƒn-keywords)

---

# 1. Tá»”NG QUAN

## ğŸ“Œ Context Rot lÃ  gÃ¬?

```
CONTEXT ROT DEFINITION:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  SOURCE: Chroma Research                           â”‚
â”‚                                                     â”‚
â”‚  DEFINITION:                                       â”‚
â”‚  "The phenomenon where LLMs exhibit DEGRADED       â”‚
â”‚   PERFORMANCE as input length increases,           â”‚
â”‚   even on SIMPLE tasks"                            â”‚
â”‚                                                     â”‚
â”‚  KEY INSIGHT:                                      â”‚
â”‚  "Models do NOT use their context uniformly;       â”‚
â”‚   instead, their performance grows increasingly    â”‚
â”‚   UNRELIABLE as input length grows"                â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Œ The Fundamental Assumption

```
THE FALSE ASSUMPTION:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  ASSUMPTION:                                       â”‚
â”‚  "LLMs with million-token context windows          â”‚
â”‚   should process information uniformly"            â”‚
â”‚                                                     â”‚
â”‚  Expected: Token 10,000 = Token 100                â”‚
â”‚            (Same reliability)                      â”‚
â”‚                                                     â”‚
â”‚  REALITY:                                          â”‚
â”‚  Performance DEGRADES significantly with length    â”‚
â”‚                                                     â”‚
â”‚  Position 100:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95% accurate   â”‚
â”‚  Position 1,000:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 85% accurate   â”‚
â”‚  Position 10,000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 70% accurate   â”‚
â”‚  Position 50,000: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 50% accurate   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 2. CORE PROBLEM

## ğŸ“Œ Beyond Simple Retrieval

```
THE BENCHMARK ILLUSION:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  NEEDLE IN A HAYSTACK (NIAH) TEST:                 â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                 â”‚
â”‚  â€¢ Hide a fact in long context                     â”‚
â”‚  â€¢ Ask model to retrieve it                        â”‚
â”‚  â€¢ Models perform WELL on this                     â”‚
â”‚                                                     â”‚
â”‚  BUT NIAH ONLY TESTS:                              â”‚
â”‚  â€¢ Simple lexical retrieval                        â”‚
â”‚  â€¢ Exact keyword matching                          â”‚
â”‚  â€¢ NOT real-world complexity                       â”‚
â”‚                                                     â”‚
â”‚  REAL-WORLD REQUIRES:                              â”‚
â”‚  â€¢ Semantic understanding                          â”‚
â”‚  â€¢ Multiple pieces of information                  â”‚
â”‚  â€¢ Reasoning across context                        â”‚
â”‚  â€¢ Generation, not just retrieval                  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Œ The Gap Between Benchmarks and Reality

```
BENCHMARK VS REALITY:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  BENCHMARK:                                        â”‚
â”‚  "Find the secret code: X7Y9Z2"                    â”‚
â”‚  â†’ Easy, exact match                               â”‚
â”‚                                                     â”‚
â”‚  REAL-WORLD:                                       â”‚
â”‚  "Summarize all security vulnerabilities           â”‚
â”‚   mentioned in this 100-page report and            â”‚
â”‚   prioritize based on our infrastructure"          â”‚
â”‚  â†’ Requires deep understanding + reasoning         â”‚
â”‚                                                     â”‚
â”‚  MODELS:                                           â”‚
â”‚  â€¢ Pass NIAH benchmark âœ“                           â”‚
â”‚  â€¢ Fail real-world tasks at same context length âœ—  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 3. NGUYÃŠN NHÃ‚N & PATTERNS

## ğŸ“Œ Key Degradation Factors

```
FACTOR 1: NEEDLE-QUESTION SIMILARITY

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  Performance degrades MORE RAPIDLY with:           â”‚
â”‚  LOWER semantic similarity between                 â”‚
â”‚  question and relevant information                 â”‚
â”‚                                                     â”‚
â”‚  HIGH SIMILARITY:                                  â”‚
â”‚  Q: "What is the API key?"                         â”‚
â”‚  A: "The API key is abc123"                        â”‚
â”‚  â†’ Model finds easily                              â”‚
â”‚                                                     â”‚
â”‚  LOW SIMILARITY:                                   â”‚
â”‚  Q: "How should we authenticate?"                  â”‚
â”‚  A: "Use abc123 for service access"                â”‚
â”‚  â†’ Model struggles more at length                  â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FACTOR 2: DISTRACTOR EFFECTS

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  "Even a SINGLE DISTRACTOR reduces performance     â”‚
â”‚   relative to the baseline"                        â”‚
â”‚                                                     â”‚
â”‚  Impact AMPLIFIES at longer input lengths          â”‚
â”‚                                                     â”‚
â”‚  With 0 distractors:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ High accuracy  â”‚
â”‚  With 1 distractor:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ Lower          â”‚
â”‚  With 5 distractors:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ Even lower     â”‚
â”‚  With 10 distractors: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ Significant dropâ”‚
â”‚                                                     â”‚
â”‚  Different distractors have NON-UNIFORM effects    â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FACTOR 3: HAYSTACK STRUCTURE

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  SURPRISING FINDING:                               â”‚
â”‚                                                     â”‚
â”‚  Models perform BETTER with:                       â”‚
â”‚  â€¢ Shuffled, INCOHERENT content                    â”‚
â”‚                                                     â”‚
â”‚  Than with:                                        â”‚
â”‚  â€¢ Logically STRUCTURED text                       â”‚
â”‚                                                     â”‚
â”‚  WHY?                                              â”‚
â”‚  Attention mechanisms behave unexpectedly          â”‚
â”‚  with coherent inputs at scale                     â”‚
â”‚                                                     â”‚
â”‚  Coherent text may "distract" the model            â”‚
â”‚  more than random content                          â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FACTOR 4: OUTPUT GENERATION DEMANDS

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  When OUTPUT length scales with INPUT length:      â”‚
â”‚                                                     â”‚
â”‚  â€¢ Models struggle to maintain position accuracy   â”‚
â”‚  â€¢ Often UNDERGENERATE content                     â”‚
â”‚  â€¢ Or HALLUCINATE content                          â”‚
â”‚                                                     â”‚
â”‚  Example task: "List all 50 items mentioned"       â”‚
â”‚  Result: Model lists 30, misses 20, adds 5 fake   â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 4. EFFECTS ON CONTEXT WINDOWS

## ğŸ“Œ Observable Effects

```
EFFECTS OF CONTEXT ROT:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  1. POSITION-DEPENDENT ACCURACY                    â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚     Information at START of context:               â”‚
â”‚     â†’ Better performance                           â”‚
â”‚                                                     â”‚
â”‚     Information at END of context:                 â”‚
â”‚     â†’ Worse performance                            â”‚
â”‚                                                     â”‚
â”‚     Information in MIDDLE:                         â”‚
â”‚     â†’ Often "lost in the middle"                   â”‚
â”‚                                                     â”‚
â”‚  2. INCREASED HALLUCINATION                        â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚     Longer contexts â†’ More hallucinations          â”‚
â”‚     Model "fills in" what it can't find            â”‚
â”‚                                                     â”‚
â”‚  3. TASK REFUSAL                                   â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚     At extreme lengths:                            â”‚
â”‚     â€¢ Some models refuse to answer                 â”‚
â”‚     â€¢ Or produce random output                     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Œ Model-Specific Behaviors

```
MODEL DIFFERENCES:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  CLAUDE MODELS:                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ Tend to ABSTAIN when uncertain                  â”‚
â”‚  â€¢ "I cannot find that information"                â”‚
â”‚  â€¢ More conservative                               â”‚
â”‚                                                     â”‚
â”‚  GPT MODELS:                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  â€¢ Tend to HALLUCINATE when uncertain              â”‚
â”‚  â€¢ Provide confident but wrong answers             â”‚
â”‚  â€¢ Less conservative                               â”‚
â”‚                                                     â”‚
â”‚  IMPLICATION:                                      â”‚
â”‚  Know your model's failure modes                   â”‚
â”‚  Design accordingly                                â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 5. CONTEXT ENGINEERING SOLUTIONS

## ğŸ“Œ The Key Insight

```
CONTEXT ENGINEERING:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  "What matters more is HOW information             â”‚
â”‚   is PRESENTED"                                    â”‚
â”‚                                                     â”‚
â”‚  NOT just: How much context can we fit?            â”‚
â”‚  BUT: How do we organize that context?             â”‚
â”‚                                                     â”‚
â”‚  EFFECTIVE ORGANIZATION = RELIABLE PERFORMANCE     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Œ Strategies

```
CONTEXT ENGINEERING STRATEGIES:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  STRATEGY 1: PRIORITIZE PLACEMENT                  â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
â”‚  â€¢ Put critical info at START and END              â”‚
â”‚  â€¢ Avoid important info in middle                  â”‚
â”‚  â€¢ Use recency effect (recent = remembered)        â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ [IMPORTANT] ... less critical   â”‚               â”‚
â”‚  â”‚ ... ... middle ... ...          â”‚               â”‚
â”‚  â”‚ less critical ... [IMPORTANT]   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  STRATEGY 2: REDUCE DISTRACTORS                    â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â”‚
â”‚  â€¢ Filter irrelevant information                   â”‚
â”‚  â€¢ Use RAG to retrieve only relevant chunks        â”‚
â”‚  â€¢ Summarize before including                      â”‚
â”‚                                                     â”‚
â”‚  Instead of: Include entire 100-page doc           â”‚
â”‚  Do: Include only relevant 5 pages                 â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  STRATEGY 3: CHUNK AND PROCESS                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â”‚
â”‚  â€¢ Break large tasks into smaller ones             â”‚
â”‚  â€¢ Process chunks separately                       â”‚
â”‚  â€¢ Aggregate results                               â”‚
â”‚                                                     â”‚
â”‚  Task: "Analyze 100 files"                         â”‚
â”‚  Better: Analyze 10 files Ã— 10 requests            â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  STRATEGY 4: EXPLICIT POINTERS                     â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â”‚
â”‚  â€¢ Add markers: "IMPORTANT:", "KEY:", etc.         â”‚
â”‚  â€¢ Use structured formats (JSON, XML)              â”‚
â”‚  â€¢ Reference by ID: "See item #7"                  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ [KEY_INFO_1] The API endpoint   â”‚               â”‚
â”‚  â”‚ is at /api/v2/users             â”‚               â”‚
â”‚  â”‚                                 â”‚               â”‚
â”‚  â”‚ Question: What is KEY_INFO_1?   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  STRATEGY 5: SEMANTIC SIMILARITY                   â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â”‚
â”‚  â€¢ Make questions similar to answers               â”‚
â”‚  â€¢ Use same terminology                            â”‚
â”‚  â€¢ Include keywords in both                        â”‚
â”‚                                                     â”‚
â”‚  Info: "Authentication uses JWT tokens"            â”‚
â”‚  Question: "What JWT tokens are used               â”‚
â”‚            for authentication?"                    â”‚
â”‚  (High keyword overlap helps retrieval)            â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Œ Production Best Practices

```
PRODUCTION RECOMMENDATIONS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  1. TEST AT SCALE                                  â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚     Don't assume short-context tests               â”‚
â”‚     predict long-context performance               â”‚
â”‚                                                     â”‚
â”‚  2. MONITOR HALLUCINATIONS                         â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚     Track output quality at different lengths      â”‚
â”‚                                                     â”‚
â”‚  3. USE RETRIEVAL AUGMENTATION                     â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚     RAG can reduce effective context length        â”‚
â”‚     by filtering to relevant info only             â”‚
â”‚                                                     â”‚
â”‚  4. IMPLEMENT FALLBACKS                            â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚     When context exceeds threshold                 â”‚
â”‚     â†’ Chunk, summarize, or refuse gracefully       â”‚
â”‚                                                     â”‚
â”‚  5. CHOOSE MODELS WISELY                           â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚     Different models degrade differently           â”‚
â”‚     Test your specific use case                    â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 6. Tá»ª ÄIá»‚N KEYWORDS

| Tá»« khÃ³a | NghÄ©a | Giáº£i thÃ­ch thÃªm |
|---------|-------|-----------------|
| **Context Rot** | Suy giáº£m context | Performance giáº£m theo context length |
| **Context Window** | Cá»­a sá»• context | Sá»‘ tokens model cÃ³ thá»ƒ xá»­ lÃ½ |
| **NIAH** | Needle in a Haystack | Test tÃ¬m thÃ´ng tin trong context dÃ i |
| **Distractor** | ThÃ´ng tin nhiá»…u | Content lÃ m model bá»‹ phÃ¢n tÃ¢m |
| **Hallucination** | áº¢o giÃ¡c | Model táº¡o ra thÃ´ng tin sai |
| **Position-dependent** | Phá»¥ thuá»™c vá»‹ trÃ­ | Performance khÃ¡c nhau theo vá»‹ trÃ­ |
| **Context Engineering** | Ká»¹ thuáº­t context | Tá»‘i Æ°u cÃ¡ch tá»• chá»©c context |
| **RAG** | Retrieval-Augmented Generation | Káº¿t há»£p search vá»›i generation |
| **Semantic Similarity** | TÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a | Äá»™ giá»‘ng vá» Ã½ nghÄ©a |
| **Lost in the Middle** | Máº¥t á»Ÿ giá»¯a | Phenomenon thÃ´ng tin bá»‹ bá» qua |

---

# ğŸ“š TÃ€I NGUYÃŠN

## Links
- [Chroma Research - Context Rot](https://research.trychroma.com/context-rot) - Nguá»“n gá»‘c
- [Lost in the Middle Paper](https://arxiv.org/abs/2307.03172) - Related research
- [Anthropic Context Guidelines](https://docs.anthropic.com/) - Best practices

---

*TÃ i liá»‡u phÃ¢n tÃ­ch Context Rot - hiá»‡n tÆ°á»£ng suy giáº£m performance trong AI context windows.*
