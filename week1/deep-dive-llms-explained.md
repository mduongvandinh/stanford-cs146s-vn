# ğŸ§  HÆ¯á»šNG DáºªN HIá»‚U VIDEO "DEEP DIVE INTO LLMs LIKE CHATGPT"
## TÃ¡c giáº£ video: Andrej Karpathy (Cá»±u Ä‘á»“ng sÃ¡ng láº­p OpenAI)

---

# ğŸ“– Má»¤C Lá»¤C

1. [LLM lÃ  gÃ¬?](#1-llm-lÃ -gÃ¬)
2. [Tá»•ng quan 3 giai Ä‘oáº¡n táº¡o LLM](#2-tá»•ng-quan-3-giai-Ä‘oáº¡n-táº¡o-llm)
3. [GIAI ÄOáº N 1: Pre-training](#3-giai-Ä‘oáº¡n-1-pre-training-huáº¥n-luyá»‡n-trÆ°á»›c)
4. [GIAI ÄOáº N 2: Supervised Fine-tuning](#4-giai-Ä‘oáº¡n-2-supervised-fine-tuning-sft)
5. [GIAI ÄOáº N 3: Reinforcement Learning](#5-giai-Ä‘oáº¡n-3-reinforcement-learning-rl)
6. [TÃ¢m lÃ½ há»c cá»§a LLM](#6-tÃ¢m-lÃ½-há»c-cá»§a-llm)
7. [CÃ¡ch sá»­ dá»¥ng LLM hiá»‡u quáº£](#7-cÃ¡ch-sá»­-dá»¥ng-llm-hiá»‡u-quáº£)
8. [Tá»« Ä‘iá»ƒn Keywords](#8-tá»«-Ä‘iá»ƒn-keywords-quan-trá»ng)

---

# 1. LLM LÃ€ GÃŒ?

## ğŸ“Œ Äá»‹nh nghÄ©a Ä‘Æ¡n giáº£n

**LLM = Large Language Model = MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n**

HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t ngÆ°á»i Ä‘Ã£ Ä‘á»c gáº§n nhÆ° **toÃ n bá»™ Internet**, nhá»› háº¿t má»i thá»©, vÃ  cÃ³ thá»ƒ viáº¿t tiáº¿p báº¥t ká»³ Ä‘oáº¡n vÄƒn nÃ o báº¡n Ä‘Æ°a ra.

---

## ğŸ“Œ PhÃ¢n tÃ­ch tá»«ng tá»« trong "Large Language Model"

| Tá»« | NghÄ©a | Giáº£i thÃ­ch |
|-----|-------|------------|
| **Large** | Lá»›n | CÃ³ hÃ ng tá»· "parameters" (tham sá»‘) - giá»‘ng nhÆ° cÃ³ hÃ ng tá»· táº¿ bÃ o nÃ£o |
| **Language** | NgÃ´n ngá»¯ | ChuyÃªn xá»­ lÃ½ text, chá»¯ viáº¿t, ngÃ´n ngá»¯ con ngÆ°á»i |
| **Model** | MÃ´ hÃ¬nh | Má»™t chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh Ä‘Æ°á»£c "huáº¥n luyá»‡n" |

**Váº­y LLM = Má»™t chÆ°Æ¡ng trÃ¬nh mÃ¡y tÃ­nh Ráº¤T Lá»šN Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ hiá»ƒu vÃ  táº¡o ra ngÃ´n ngá»¯.**

---

## ğŸ“Œ "NgÆ°á»i Ä‘Ã£ Ä‘á»c gáº§n nhÆ° toÃ n bá»™ Internet" - NghÄ©a lÃ  gÃ¬?

### So sÃ¡nh lÆ°á»£ng "Ä‘á»c":

**Con ngÆ°á»i bÃ¬nh thÆ°á»ng Ä‘á»c Ä‘Æ°á»£c bao nhiÃªu?**
```
- Má»™t ngÆ°á»i Ä‘á»c nhiá»u: ~1,000 cuá»‘n sÃ¡ch trong Ä‘á»i
- 1 cuá»‘n sÃ¡ch â‰ˆ 50,000 tá»«
- Tá»•ng: ~50 triá»‡u tá»« trong cáº£ Ä‘á»i
```

**LLM "Ä‘á»c" Ä‘Æ°á»£c bao nhiÃªu?**
```
- GPT-4 Ä‘Æ°á»£c train trÃªn: ~13 nghÃ¬n tá»· tokens (Æ°á»›c tÃ­nh)
- 1 token â‰ˆ 0.75 tá»«
- Tá»•ng: ~10 nghÃ¬n tá»· tá»«

â†’ LLM "Ä‘á»c" gáº¥p 200,000 láº§n má»™t ngÆ°á»i Ä‘á»c nhiá»u nháº¥t
```

### Cá»¥ thá»ƒ LLM Ä‘Ã£ "Ä‘á»c" nhá»¯ng gÃ¬?

| Nguá»“n | VÃ­ dá»¥ |
|-------|-------|
| Wikipedia | Háº§u háº¿t cÃ¡c bÃ i viáº¿t |
| SÃ¡ch | HÃ ng triá»‡u cuá»‘n sÃ¡ch Ä‘Ã£ sá»‘ hÃ³a |
| Website | HÃ ng tá»· trang web |
| Forum | Reddit, StackOverflow, Quora... |
| Code | GitHub, GitLab... |
| BÃ¡o chÃ­ | NYT, BBC, VnExpress... |
| Paper khoa há»c | ArXiv, PubMed... |

---

## ğŸ“Œ "Nhá»› háº¿t má»i thá»©" - ÄÃºng nhÆ°ng KHÃC vá»›i con ngÆ°á»i

### Con ngÆ°á»i nhá»› nhÆ° tháº¿ nÃ o?
```
Báº¡n Ä‘á»c: "Paris lÃ  thá»§ Ä‘Ã´ cá»§a PhÃ¡p"
NÃ£o báº¡n: LÆ°u fact nÃ y vÃ o bá»™ nhá»›
Khi há»i: Truy xuáº¥t fact Ä‘Ã³ ra
```

### LLM "nhá»›" nhÆ° tháº¿ nÃ o?
```
LLM Ä‘á»c: HÃ ng triá»‡u cÃ¢u chá»©a "Paris" vÃ  "thá»§ Ä‘Ã´" vÃ  "PhÃ¡p"
LLM há»c: Pattern - khi tháº¥y "thá»§ Ä‘Ã´ cá»§a PhÃ¡p", 
         tá»« tiáº¿p theo cÃ³ xÃ¡c suáº¥t cao lÃ  "Paris"
         
â†’ LLM KHÃ”NG lÆ°u fact, mÃ  lÆ°u PATTERN (quy luáº­t)
```

### VÃ­ dá»¥ minh há»a sá»± khÃ¡c biá»‡t:

```
Há»i: "Thá»§ Ä‘Ã´ cá»§a PhÃ¡p lÃ  gÃ¬?"

CON NGÆ¯á»œI suy nghÄ©:
"Ã€, tÃ´i Ä‘Ã£ há»c Ä‘iá»u nÃ y... thá»§ Ä‘Ã´ PhÃ¡p... Paris!"
(Truy xuáº¥t fact tá»« bá»™ nhá»›)

LLM "suy nghÄ©":
"Sau cá»¥m 'Thá»§ Ä‘Ã´ cá»§a PhÃ¡p lÃ ', token nÃ o cÃ³ xÃ¡c suáº¥t cao nháº¥t?"
â†’ TÃ­nh toÃ¡n: "Paris" = 95%, "thÃ nh phá»‘" = 2%, "nÆ¡i" = 1%...
â†’ Chá»n "Paris"
(Dá»± Ä‘oÃ¡n dá»±a trÃªn pattern Ä‘Ã£ há»c)
```

### âš ï¸ ÄÃ¢y lÃ  lÃ½ do LLM cÃ³ thá»ƒ "hallucinate" (bá»‹a):
- **Con ngÆ°á»i:** KhÃ´ng biáº¿t â†’ NÃ³i "tÃ´i khÃ´ng biáº¿t"
- **LLM:** KhÃ´ng cÃ³ pattern rÃµ rÃ ng â†’ Váº«n Ä‘oÃ¡n token cÃ³ xÃ¡c suáº¥t cao nháº¥t â†’ CÃ³ thá»ƒ sai

---

## ğŸ“Œ "Viáº¿t tiáº¿p báº¥t ká»³ Ä‘oáº¡n vÄƒn nÃ o" - Báº¢N CHáº¤T cá»§a LLM

### Nhiá»‡m vá»¥ DUY NHáº¤T cá»§a LLM: Dá»± Ä‘oÃ¡n token tiáº¿p theo

HÃ£y xem vÃ­ dá»¥ tá»«ng bÆ°á»›c:

```
Input: "HÃ´m nay trá»i"
                    â†“
            [LLM tÃ­nh toÃ¡n]
                    â†“
XÃ¡c suáº¥t token tiáº¿p theo:
- "Ä‘áº¹p" : 35%
- "mÆ°a" : 25%  
- "náº¯ng" : 20%
- "xanh" : 10%
- "ráº¥t" : 5%
- ... (100,000 tokens khÃ¡c)
                    â†“
LLM chá»n "Ä‘áº¹p" (hoáº·c random theo xÃ¡c suáº¥t)
                    â†“
Output: "HÃ´m nay trá»i Ä‘áº¹p"
```

### Sau Ä‘Ã³ láº·p láº¡i:
```
Input: "HÃ´m nay trá»i Ä‘áº¹p"
                    â†“
            [LLM tÃ­nh toÃ¡n]
                    â†“
XÃ¡c suáº¥t token tiáº¿p theo:
- "quÃ¡" : 30%
- "," : 25%
- "láº¯m" : 15%
- ...
                    â†“
LLM chá»n "quÃ¡"
                    â†“
Output: "HÃ´m nay trá»i Ä‘áº¹p quÃ¡"
```

**Cá»© tháº¿ láº·p láº¡i cho Ä‘áº¿n khi gáº·p token káº¿t thÃºc hoáº·c Ä‘áº¡t giá»›i háº¡n.**

---

## ğŸ“Œ VÃ­ dá»¥ thá»±c táº¿ Ä‘á»ƒ hiá»ƒu sÃ¢u hÆ¡n

### VÃ­ dá»¥ 1: LLM nhÆ° "autocomplete siÃªu cáº¥p"

Báº¡n biáº¿t tÃ­nh nÄƒng gá»£i Ã½ khi gÃµ tin nháº¯n trÃªn Ä‘iá»‡n thoáº¡i khÃ´ng?

```
Báº¡n gÃµ: "TÃ´i Ä‘ang á»Ÿ"
Äiá»‡n thoáº¡i gá»£i Ã½: "nhÃ " | "Ä‘Ã¢y" | "cÃ´ng ty"
```

**LLM cÅ©ng nhÆ° váº­y, nhÆ°ng:**
- **Äiá»‡n thoáº¡i:** Dá»±a trÃªn vÃ i tá»« trÆ°á»›c + thÃ³i quen cá»§a báº¡n
- **LLM:** Dá»±a trÃªn TOÃ€N Bá»˜ ngá»¯ cáº£nh + kiáº¿n thá»©c tá»« Internet

```
Input: "Theo thuyáº¿t tÆ°Æ¡ng Ä‘á»‘i cá»§a Einstein, khi váº­n tá»‘c 
        tiáº¿n gáº§n tá»‘c Ä‘á»™ Ã¡nh sÃ¡ng thÃ¬"

Äiá»‡n thoáº¡i: ???
LLM: "thá»i gian sáº½ cháº­m láº¡i vÃ  khá»‘i lÆ°á»£ng tÄƒng lÃªn"
```

### VÃ­ dá»¥ 2: Táº¡i sao gá»i lÃ  "viáº¿t tiáº¿p" chá»© khÃ´ng pháº£i "tráº£ lá»i"?

**Base Model (chÆ°a fine-tune) thá»±c sá»± chá»‰ biáº¿t viáº¿t tiáº¿p:**

```
Input: "User: 2+2 báº±ng máº¥y?
        Assistant:"

Base Model output: "User: 3+3 báº±ng máº¥y?
                    Assistant:
                    User: 4+4 báº±ng máº¥y?
                    Assistant:..."
                    
â†’ NÃ³ VIáº¾T TIáº¾P cÃ¡i pattern há»i-Ä‘Ã¡p, khÃ´ng thá»±c sá»± TRáº¢ Lá»œI
```

**Sau khi Fine-tune má»›i biáº¿t tráº£ lá»i:**
```
Input: "User: 2+2 báº±ng máº¥y?
        Assistant:"

Fine-tuned Model: "4"

â†’ ÄÃ£ há»c Ä‘Æ°á»£c: Sau "Assistant:" pháº£i lÃ  CÃ‚U TRáº¢ Lá»œI
```

### VÃ­ dá»¥ 3: "Nhá»›" theo pattern, khÃ´ng pháº£i fact

```
LLM Ä‘Ã£ "Ä‘á»c" hÃ ng triá»‡u cÃ¢u nhÆ°:
- "The capital of France is Paris"
- "Paris is the capital of France"  
- "France's capital city is Paris"
- "What is the capital of France? Paris"
- "Thá»§ Ä‘Ã´ cá»§a PhÃ¡p lÃ  Paris"
- ...

LLM KHÃ”NG lÆ°u: fact(France, capital) = Paris
LLM LÆ¯U: Khi tháº¥y pattern "capital" + "France" â†’ "Paris" cÃ³ xÃ¡c suáº¥t cao

ÄÃ¢y lÃ  lÃ½ do:
âœ… Há»i cÃ¡ch khÃ¡c váº«n Ä‘Ãºng (vÃ¬ pattern tÆ°Æ¡ng tá»±)
âŒ NhÆ°ng há»i vá» fact hiáº¿m cÃ³ thá»ƒ sai (khÃ´ng Ä‘á»§ pattern)
```

---

## ğŸ“Œ So sÃ¡nh: LLM vs Con ngÆ°á»i vs Google Search

| KhÃ­a cáº¡nh | Con ngÆ°á»i | Google Search | LLM |
|-----------|-----------|---------------|-----|
| CÃ¡ch "biáº¿t" | Há»c vÃ  nhá»› facts | Index vÃ  tÃ¬m kiáº¿m | Há»c patterns |
| Khi khÃ´ng biáº¿t | NÃ³i "khÃ´ng biáº¿t" | NÃ³i "khÃ´ng tÃ¬m tháº¥y" | CÃ³ thá»ƒ bá»‹a (hallucinate) |
| SÃ¡ng táº¡o | CÃ³ | KhÃ´ng | CÃ³ (theo pattern) |
| Cáº­p nháº­t | LiÃªn tá»¥c | Real-time | Cá»‘ Ä‘á»‹nh (cutoff date) |
| Giáº£i thÃ­ch | Hiá»ƒu sÃ¢u | Chá»‰ tÃ¬m kiáº¿m | Báº¯t chÆ°á»›c giáº£i thÃ­ch |

---

## ğŸ“Œ CÃ¡c LLM phá»• biáº¿n hiá»‡n nay

| TÃªn | CÃ´ng ty | Loáº¡i |
|-----|---------|------|
| ChatGPT (GPT-4, GPT-4o) | OpenAI | ÄÃ³ng (Proprietary) |
| Claude | Anthropic | ÄÃ³ng |
| Gemini | Google | ÄÃ³ng |
| Llama | Meta | Má»Ÿ (Open-weights) |
| DeepSeek | DeepSeek | Má»Ÿ |

---

## ğŸ“Œ TÃ“M Táº®T

**LLM = MÃ¡y dá»± Ä‘oÃ¡n token siÃªu cáº¥p** Ä‘Æ°á»£c train trÃªn gáº§n nhÆ° toÃ n bá»™ text cá»§a Internet.

NÃ³ khÃ´ng thá»±c sá»± "hiá»ƒu" hay "nhá»›" nhÆ° con ngÆ°á»i, mÃ  há»c Ä‘Æ°á»£c **patterns** (quy luáº­t) trong ngÃ´n ngá»¯. Khi báº¡n Ä‘Æ°a vÃ o má»™t Ä‘oáº¡n text, nÃ³ dá»± Ä‘oÃ¡n token tiáº¿p theo cÃ³ xÃ¡c suáº¥t cao nháº¥t, rá»“i láº·p láº¡i cho Ä‘áº¿n khi hoÃ n thÃ nh cÃ¢u tráº£ lá»i.

**VÃ­ von cuá»‘i cÃ¹ng:**
- **Con ngÆ°á»i:** Äá»c sÃ¡ch â†’ Hiá»ƒu â†’ Nhá»› kiáº¿n thá»©c â†’ Tráº£ lá»i tá»« kiáº¿n thá»©c
- **LLM:** "Äá»c" Internet â†’ Há»c pattern â†’ Khi há»i, Ä‘oÃ¡n tá»« tiáº¿p theo giá»‘ng pattern Ä‘Ã£ há»c

---

# 2. Tá»”NG QUAN 3 GIAI ÄOáº N Táº O LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUY TRÃŒNH Táº O LLM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  GIAI ÄOáº N 1          GIAI ÄOáº N 2          GIAI ÄOáº N 3         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚                                                                 â”‚
â”‚  PRE-TRAINING    â†’    FINE-TUNING     â†’    REINFORTIC          â”‚
â”‚  (Huáº¥n luyá»‡n          (Tinh chá»‰nh          LEARNING            â”‚
â”‚   trÆ°á»›c)               cÃ³ giÃ¡m sÃ¡t)        (Há»c tÄƒng cÆ°á»ng)    â”‚
â”‚                                                                 â”‚
â”‚  â†“                    â†“                    â†“                   â”‚
â”‚                                                                 â”‚
â”‚  BASE MODEL      â†’    ASSISTANT       â†’    REFINED             â”‚
â”‚  (Biáº¿t nhiá»u          MODEL                MODEL               â”‚
â”‚   nhÆ°ng chÆ°a          (Biáº¿t tráº£ lá»i)       (Tráº£ lá»i tá»‘t,       â”‚
â”‚   biáº¿t tráº£ lá»i)                            an toÃ n)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### So sÃ¡nh dá»… hiá»ƒu:

| Giai Ä‘oáº¡n | VÃ­ dá»¥ vá»›i con ngÆ°á»i |
|-----------|---------------------|
| Pre-training | Äá»©a tráº» Ä‘á»c háº¿t thÆ° viá»‡n, biáº¿t má»i thá»© |
| Fine-tuning | Dáº¡y Ä‘á»©a tráº» cÃ¡ch tráº£ lá»i cÃ¢u há»i lá»‹ch sá»± |
| Reinforcement Learning | Khen/chÃª Ä‘á»ƒ Ä‘á»©a tráº» tráº£ lá»i ngÃ y cÃ ng tá»‘t hÆ¡n |

---

# 3. GIAI ÄOáº N 1: PRE-TRAINING (Huáº¥n luyá»‡n trÆ°á»›c)

## ğŸ¯ Má»¥c tiÃªu
Cho model "Ä‘á»c" hÃ ng tá»· trang web Ä‘á»ƒ há»c kiáº¿n thá»©c vÃ  ngÃ´n ngá»¯.

---

## BÆ¯á»šC 1.1: THU THáº¬P Dá»® LIá»†U (Data Collection)

### ğŸ“Œ Keyword: Common Crawl, FineWeb

**Common Crawl lÃ  gÃ¬?**
- Tá»• chá»©c phi lá»£i nhuáº­n thu tháº­p dá»¯ liá»‡u tá»« Internet
- Chá»©a hÃ ng tá»· trang web
- Ai cÅ©ng cÃ³ thá»ƒ táº£i vá» miá»…n phÃ­

**FineWeb lÃ  gÃ¬?**
- Dataset Ä‘Ã£ Ä‘Æ°á»£c lá»c sáº¡ch tá»« Common Crawl
- Khoáº£ng 44 terabytes text
- Khoáº£ng 15 nghÃ¬n tá»· (trillion) tokens

### ğŸ“Œ VÃ­ dá»¥ trá»±c quan

```
Internet (Petabytes)
    â†“ Thu tháº­p
Common Crawl (Terabytes) 
    â†“ Lá»c sáº¡ch
FineWeb (44 TB, 15T tokens)
    â†“ DÃ¹ng Ä‘á»ƒ train
LLM
```

---

## BÆ¯á»šC 1.2: Lá»ŒC VÃ€ LÃ€M Sáº CH (Data Filtering)

### ğŸ“Œ Keyword: URL Filtering, Deduplication, PII Removal

**Táº¡i sao pháº£i lá»c?**
Internet cÃ³ ráº¥t nhiá»u rÃ¡c: spam, virus, ná»™i dung trÃ¹ng láº·p, thÃ´ng tin cÃ¡ nhÃ¢n...

### ğŸ“Œ CÃ¡c bÆ°á»›c lá»c:

| BÆ°á»›c | TÃªn tiáº¿ng Anh | Giáº£i thÃ­ch |
|------|---------------|------------|
| 1 | URL Filtering | Loáº¡i bá» cÃ¡c website spam, malware, cháº¥t lÆ°á»£ng tháº¥p |
| 2 | Text Extraction | Láº¥y text tá»« HTML, bá» code, quáº£ng cÃ¡o |
| 3 | Language Filtering | Chá»‰ giá»¯ ngÃ´n ngá»¯ cáº§n thiáº¿t (vÃ­ dá»¥: tiáº¿ng Anh) |
| 4 | Deduplication | Loáº¡i bá» ná»™i dung trÃ¹ng láº·p |
| 5 | PII Removal | XÃ³a thÃ´ng tin cÃ¡ nhÃ¢n (sá»‘ Ä‘iá»‡n thoáº¡i, email, Ä‘á»‹a chá»‰) |
| 6 | Quality Filtering | Chá»‰ giá»¯ ná»™i dung cháº¥t lÆ°á»£ng cao |

### ğŸ“Œ VÃ­ dá»¥ PII Removal

```
TRÆ¯á»šC: "LiÃªn há»‡ John Smith, email: john@gmail.com, SÄT: 0901234567"
SAU:   "LiÃªn há»‡ [NAME], email: [EMAIL], SÄT: [PHONE]"
```

---

## BÆ¯á»šC 1.3: TOKENIZATION (MÃ£ hÃ³a thÃ nh Token)

### ğŸ“Œ Keyword: Token, BPE (Byte Pair Encoding), Vocabulary

**Token lÃ  gÃ¬?**
- Token lÃ  "máº£nh" nhá» cá»§a text
- KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng lÃ  1 tá»«
- CÃ³ thá»ƒ lÃ : 1 tá»«, 1 pháº§n tá»«, 1 kÃ½ tá»±, hoáº·c nhiá»u tá»«

### ğŸ“Œ VÃ­ dá»¥ Tokenization

```
CÃ¢u: "ChatGPT is amazing"

CÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh:
["Chat", "G", "PT", " is", " amazing"]
   â†“       â†“    â†“      â†“        â†“
Token 1   T2   T3     T4       T5

Má»—i token Ä‘Æ°á»£c gÃ¡n 1 sá»‘ ID:
[15496, 38, 2898, 318, 4998]
```

### ğŸ“Œ Táº¡i sao dÃ¹ng Token thay vÃ¬ tá»«?

| CÃ¡ch | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------|---------|------------|
| DÃ¹ng tá»« | Dá»… hiá»ƒu | Tá»« Ä‘iá»ƒn quÃ¡ lá»›n, khÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c tá»« má»›i |
| DÃ¹ng kÃ½ tá»± | Tá»« Ä‘iá»ƒn nhá» | CÃ¢u quÃ¡ dÃ i, khÃ³ há»c pattern |
| DÃ¹ng token (BPE) | CÃ¢n báº±ng cáº£ hai | ÄÃ´i khi khÃ³ debug |

### ğŸ“Œ Vocabulary Size (KÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn)

- GPT-2: ~50,000 tokens
- GPT-4: ~100,000 tokens
- Llama 3: ~128,000 tokens

---

## BÆ¯á»šC 1.4: HUáº¤N LUYá»†N NEURAL NETWORK

### ğŸ“Œ Keyword: Transformer, Parameters, Next Token Prediction

**Transformer lÃ  gÃ¬?**
- Kiáº¿n trÃºc neural network Ä‘Æ°á»£c Google giá»›i thiá»‡u nÄƒm 2017
- BÃ i bÃ¡o ná»•i tiáº¿ng: "Attention Is All You Need"
- Táº¥t cáº£ LLM hiá»‡n Ä‘áº¡i Ä‘á»u dÃ¹ng Transformer

**Parameters (Tham sá»‘) lÃ  gÃ¬?**
- LÃ  cÃ¡c "nÃºt váº·n" trong model
- Model há»c = Ä‘iá»u chá»‰nh cÃ¡c nÃºt nÃ y
- GPT-2: 1.6 tá»· parameters
- GPT-4: Æ°á»›c tÃ­nh 1.8 nghÃ¬n tá»· parameters
- Llama 3.1: 405 tá»· parameters

### ğŸ“Œ CÃ¡ch model há»c

```
Input:  "HÃ  Ná»™i lÃ  thá»§ Ä‘Ã´ cá»§a"
         â†“
    [TRANSFORMER]
         â†“
Output: XÃ¡c suáº¥t cho tá»«ng token tiáº¿p theo
        - "Viá»‡t Nam": 85%
        - "nÆ°á»›c": 5%
        - "thÃ nh phá»‘": 3%
        - ... (100,000 tokens khÃ¡c)
```

Model chá»n token cÃ³ xÃ¡c suáº¥t cao nháº¥t (hoáº·c random theo xÃ¡c suáº¥t).

### ğŸ“Œ QuÃ¡ trÃ¬nh training

```
Láº·p láº¡i hÃ ng Tá»¶ láº§n:

1. Láº¥y 1 Ä‘oáº¡n text tá»« dataset
2. ÄÆ°a vÃ o model, yÃªu cáº§u Ä‘oÃ¡n tá»« tiáº¿p theo
3. So sÃ¡nh vá»›i Ä‘Ã¡p Ã¡n Ä‘Ãºng
4. Äiá»u chá»‰nh parameters Ä‘á»ƒ láº§n sau Ä‘oÃ¡n tá»‘t hÆ¡n
   (QuÃ¡ trÃ¬nh nÃ y gá»i lÃ  Backpropagation)
```

### ğŸ“Œ Chi phÃ­ training

| Model | NÄƒm | Chi phÃ­ Æ°á»›c tÃ­nh |
|-------|-----|------------------|
| GPT-2 | 2019 | ~$40,000 |
| GPT-2 (reproduce vá»›i llm.c) | 2024 | ~$672 |
| GPT-4 | 2023 | ~$100 triá»‡u |
| Llama 3.1 405B | 2024 | ~$500 triá»‡u |

---

## BÆ¯á»šC 1.5: Káº¾T QUáº¢ - BASE MODEL

### ğŸ“Œ Keyword: Base Model, Completion Model

**Base Model lÃ  gÃ¬?**
- Model chá»‰ biáº¿t "viáº¿t tiáº¿p" text
- KHÃ”NG biáº¿t tráº£ lá»i cÃ¢u há»i
- KHÃ”NG biáº¿t Ä‘á»‘i thoáº¡i

### ğŸ“Œ VÃ­ dá»¥ Base Model

```
Input:  "Thá»§ Ä‘Ã´ cá»§a Viá»‡t Nam lÃ "
Output: "HÃ  Ná»™i. HÃ  Ná»™i náº±m á»Ÿ miá»n Báº¯c Viá»‡t Nam, 
         bÃªn bá» sÃ´ng Há»“ng. ThÃ nh phá»‘ cÃ³ lá»‹ch sá»­ 
         hÆ¡n 1000 nÄƒm..."
         (Viáº¿t tiáº¿p nhÆ° Wikipedia)

Input:  "User: Thá»§ Ä‘Ã´ Viá»‡t Nam lÃ  gÃ¬?
         Assistant:"
Output: "User: Thá»§ Ä‘Ã´ cá»§a PhÃ¡p lÃ  gÃ¬?
         Assistant: Paris
         User: Thá»§ Ä‘Ã´ cá»§a Nháº­t lÃ  gÃ¬?..."
         (Viáº¿t tiáº¿p pattern, KHÃ”NG tráº£ lá»i)
```

**Base Model giá»‘ng nhÆ° ngÆ°á»i biáº¿t má»i thá»© nhÆ°ng chá»‰ biáº¿t "ká»ƒ chuyá»‡n", khÃ´ng biáº¿t "tráº£ lá»i".**

---

# 4. GIAI ÄOáº N 2: SUPERVISED FINE-TUNING (SFT)

## ğŸ¯ Má»¥c tiÃªu
Dáº¡y Base Model cÃ¡ch tráº£ lá»i cÃ¢u há»i vÃ  Ä‘á»‘i thoáº¡i nhÆ° con ngÆ°á»i.

---

## BÆ¯á»šC 2.1: Táº O Dá»® LIá»†U Há»˜I THOáº I

### ğŸ“Œ Keyword: Conversation Data, Human Labelers, Chat Template

**Human Labelers lÃ  ai?**
- NgÆ°á»i Ä‘Æ°á»£c thuÃª Ä‘á»ƒ viáº¿t cÃ¡c cuá»™c há»™i thoáº¡i máº«u
- ThÆ°á»ng lÃ  ngÆ°á»i cÃ³ chuyÃªn mÃ´n trong nhiá»u lÄ©nh vá»±c
- LÆ°Æ¡ng: $15-50/giá» tÃ¹y cÃ´ng ty vÃ  Ä‘á»™ phá»©c táº¡p

### ğŸ“Œ VÃ­ dá»¥ dá»¯ liá»‡u há»™i thoáº¡i

```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Thá»§ Ä‘Ã´ Viá»‡t Nam lÃ  gÃ¬?"},
    {"role": "assistant", "content": "Thá»§ Ä‘Ã´ cá»§a Viá»‡t Nam lÃ  HÃ  Ná»™i."}
  ]
}
```

### ğŸ“Œ Chat Template / ChatML

```
<|im_start|>system
You are a helpful assistant.
<|im_end|>
<|im_start|>user
Thá»§ Ä‘Ã´ Viá»‡t Nam lÃ  gÃ¬?
<|im_end|>
<|im_start|>assistant
Thá»§ Ä‘Ã´ cá»§a Viá»‡t Nam lÃ  HÃ  Ná»™i.
<|im_end|>
```

**CÃ¡c special tokens:**
- `<|im_start|>`: Báº¯t Ä‘áº§u message
- `<|im_end|>`: Káº¿t thÃºc message
- `system`, `user`, `assistant`: Vai trÃ²

---

## BÆ¯á»šC 2.2: FINE-TUNING

### ğŸ“Œ Keyword: Fine-tuning, Supervised Learning

**QuÃ¡ trÃ¬nh:**

```
1. Láº¥y Base Model (Ä‘Ã£ train á»Ÿ giai Ä‘oáº¡n 1)
2. Tiáº¿p tá»¥c train trÃªn dá»¯ liá»‡u há»™i thoáº¡i
3. Model há»c cÃ¡ch:
   - Nháº­n diá»‡n khi nÃ o user há»i
   - Tráº£ lá»i Ä‘Ãºng format
   - Dá»«ng Ä‘Ãºng lÃºc (khÃ´ng viáº¿t tiáº¿p vÃ´ táº­n)
```

### ğŸ“Œ So sÃ¡nh trÆ°á»›c/sau Fine-tuning

| | Base Model | After Fine-tuning |
|--|-----------|-------------------|
| Input | "User: 2+2=?" | "User: 2+2=?" |
| Output | "User: 3+3=? User: 4+4=?..." | "4" |
| HÃ nh vi | Viáº¿t tiáº¿p pattern | Tráº£ lá»i cÃ¢u há»i |

---

## BÆ¯á»šC 2.3: Káº¾T QUáº¢ - ASSISTANT MODEL

### ğŸ“Œ Keyword: Assistant Model, Instruct Model

Sau fine-tuning, ta cÃ³:
- **Assistant Model**: Biáº¿t Ä‘á»‘i thoáº¡i
- **Instruct Model**: Biáº¿t lÃ m theo hÆ°á»›ng dáº«n

**ÄÃ¢y chÃ­nh lÃ  dáº¡ng model báº¡n dÃ¹ng hÃ ng ngÃ y (ChatGPT, Claude).**

---

# 5. GIAI ÄOáº N 3: REINFORCEMENT LEARNING (RL)

## ğŸ¯ Má»¥c tiÃªu
LÃ m model tráº£ lá»i Tá»T HÆ N, AN TOÃ€N HÆ N, PHÃ™ Há»¢P HÆ N vá»›i mong muá»‘n con ngÆ°á»i.

---

## BÆ¯á»šC 3.1: RLHF (Reinforcement Learning from Human Feedback)

### ğŸ“Œ Keyword: RLHF, Reward Model, Human Preference

**RLHF lÃ  gÃ¬?**
- PhÆ°Æ¡ng phÃ¡p dÃ¹ng feedback cá»§a ngÆ°á»i Ä‘á»ƒ cáº£i thiá»‡n model
- ÄÆ°á»£c OpenAI phá»• biáº¿n vá»›i ChatGPT

### ğŸ“Œ Quy trÃ¬nh RLHF

```
BÆ¯á»šC 1: Thu tháº­p Human Preferences
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User question: "Giáº£i thÃ­ch AI cho tráº» 5 tuá»•i"

Response A: "AI lÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o, má»™t lÄ©nh vá»±c 
            cá»§a khoa há»c mÃ¡y tÃ­nh..."
Response B: "AI giá»‘ng nhÆ° dáº¡y mÃ¡y tÃ­nh suy nghÄ©! 
            NhÆ° khi con dáº¡y chÃº chÃ³ ngá»“i..."

NgÆ°á»i Ä‘Ã¡nh giÃ¡: B tá»‘t hÆ¡n A âœ“


BÆ¯á»šC 2: Train Reward Model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- DÃ¹ng data preferences Ä‘á»ƒ train 1 model riÃªng
- Model nÃ y cho Ä‘iá»ƒm: Response nÃ o tá»‘t hÆ¡n?


BÆ¯á»šC 3: Optimize LLM vá»›i Reward Model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- LLM generate response
- Reward Model cho Ä‘iá»ƒm
- LLM Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ tÄƒng Ä‘iá»ƒm
- Láº·p láº¡i nhiá»u láº§n
```

### ğŸ“Œ VÃ­ dá»¥ trá»±c quan

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    LLM      â”‚
                    â”‚  (Student)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    Generate Response
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Reward    â”‚
                    â”‚   Model     â”‚
                    â”‚  (Teacher)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    Score: 0.85/1.0
                           â”‚
                           â–¼
                    Feedback to LLM
                    "LÃ m tá»‘t láº¯m!" hoáº·c
                    "Cáº§n cáº£i thiá»‡n..."
```

---

## BÆ¯á»šC 3.2: REASONING MODELS (MÃ´ hÃ¬nh suy luáº­n)

### ğŸ“Œ Keyword: Chain-of-Thought, O1, O3, DeepSeek-R1, Thinking Tokens

**Reasoning Model lÃ  gÃ¬?**
- Model Ä‘Æ°á»£c train Ä‘á»ƒ "suy nghÄ©" trÆ°á»›c khi tráº£ lá»i
- Táº¡o ra chain-of-thought (chuá»—i suy nghÄ©)
- VÃ­ dá»¥: OpenAI O1, O3, DeepSeek-R1

### ğŸ“Œ So sÃ¡nh Model thÆ°á»ng vs Reasoning Model

**Model thÆ°á»ng:**
```
User: 17 x 23 = ?
Assistant: 391
```

**Reasoning Model:**
```
User: 17 x 23 = ?
Assistant: 
<thinking>
Äá»ƒ tÃ­nh 17 x 23, tÃ´i sáº½ chia nhá»:
- 17 x 20 = 340
- 17 x 3 = 51
- 340 + 51 = 391
Kiá»ƒm tra láº¡i: 391 / 17 = 23 âœ“
</thinking>
Káº¿t quáº£: 391
```

### ğŸ“Œ Táº¡i sao DeepSeek-R1 quan trá»ng?

1. **CÃ´ng khai phÆ°Æ¡ng phÃ¡p**: TrÆ°á»›c Ä‘Ã³ OpenAI giá»¯ bÃ­ máº­t
2. **"Aha moment"**: Model tá»± khÃ¡m phÃ¡ cÃ¡ch giáº£i quyáº¿t váº¥n Ä‘á»
3. **Chi phÃ­ tháº¥p**: Train vá»›i ngÃ¢n sÃ¡ch nhá» hÆ¡n nhiá»u

### ğŸ“Œ VÃ­ dá»¥ "Aha Moment" tá»« DeepSeek paper

```
Trong quÃ¡ trÃ¬nh RL, model tá»± phÃ¡t hiá»‡n:
"Wait, maybe I need to rethink this..."
"Let me verify my calculation..."

â†’ KhÃ´ng ai dáº¡y model lÃ m Ä‘iá»u nÃ y
â†’ Model tá»± há»c Ä‘Æ°á»£c tá»« RL
```

---

# 6. TÃ‚M LÃ Há»ŒC Cá»¦A LLM

Pháº§n nÃ y Karpathy giáº£i thÃ­ch cÃ¡ch "suy nghÄ©" cá»§a LLM - ráº¥t quan trá»ng Ä‘á»ƒ sá»­ dá»¥ng hiá»‡u quáº£.

---

## 6.1 HALLUCINATION (áº¢o giÃ¡c)

### ğŸ“Œ Keyword: Hallucination, Confabulation

**Hallucination lÃ  gÃ¬?**
- LLM "bá»‹a" thÃ´ng tin khÃ´ng cÃ³ tháº­t
- Nghe ráº¥t tá»± tin, thuyáº¿t phá»¥c
- NhÆ°ng hoÃ n toÃ n sai sá»± tháº­t

### ğŸ“Œ VÃ­ dá»¥ Hallucination

```
User: Cho tÃ´i biáº¿t vá» cuá»‘n sÃ¡ch "The AI Revolution" 
      cá»§a tÃ¡c giáº£ John Smith xuáº¥t báº£n nÄƒm 2019?

LLM:  "The AI Revolution" cá»§a John Smith lÃ  cuá»‘n sÃ¡ch 
      best-seller vá» trÃ­ tuá»‡ nhÃ¢n táº¡o. SÃ¡ch gá»“m 12 
      chÆ°Æ¡ng, Ä‘á» cáº­p Ä‘áº¿n machine learning, deep 
      learning, vÃ  tÆ°Æ¡ng lai cá»§a AI...
      
â†’ SÃ¡ch nÃ y KHÃ”NG Tá»’N Táº I
â†’ LLM bá»‹a ra toÃ n bá»™ ná»™i dung
```

### ğŸ“Œ Táº¡i sao LLM hallucinate?

```
LLM khÃ´ng "biáº¿t" theo nghÄ©a con ngÆ°á»i hiá»ƒu.
LLM chá»‰ dá»± Ä‘oÃ¡n: "Token nÃ o cÃ³ xÃ¡c suáº¥t cao nháº¥t tiáº¿p theo?"

Khi khÃ´ng cÃ³ thÃ´ng tin â†’ váº«n pháº£i Ä‘oÃ¡n â†’ bá»‹a ra
```

### ğŸ“Œ CÃ¡ch giáº£m Hallucination

| CÃ¡ch | Giáº£i thÃ­ch |
|------|------------|
| YÃªu cáº§u trÃ­ch dáº«n nguá»“n | "Tráº£ lá»i vÃ  cho tÃ´i link nguá»“n" |
| Cho phÃ©p nÃ³i "khÃ´ng biáº¿t" | "Náº¿u khÃ´ng cháº¯c, hÃ£y nÃ³i khÃ´ng biáº¿t" |
| DÃ¹ng RAG | Cung cáº¥p context chÃ­nh xÃ¡c cho LLM |
| Fact-check | LuÃ´n kiá»ƒm tra thÃ´ng tin quan trá»ng |

---

## 6.2 KNOWLEDGE vs WORKING MEMORY

### ğŸ“Œ Keyword: Knowledge, Working Memory, Context Window

**Hai loáº¡i "bá»™ nhá»›" cá»§a LLM:**

| Loáº¡i | Knowledge | Working Memory |
|------|-----------|----------------|
| LÃ  gÃ¬? | Kiáº¿n thá»©c há»c tá»« training | Ná»™i dung trong cuá»™c trÃ² chuyá»‡n hiá»‡n táº¡i |
| Khi nÃ o cÃ³? | Cá»‘ Ä‘á»‹nh sau khi train | Thay Ä‘á»•i má»—i lÆ°á»£t chat |
| Giá»›i háº¡n? | Ráº¥t lá»›n nhÆ°ng cÃ³ thá»ƒ outdated | Giá»›i háº¡n bá»Ÿi context window |
| VÃ­ dá»¥ | "Paris lÃ  thá»§ Ä‘Ã´ PhÃ¡p" | "User vá»«a nÃ³i tÃªn lÃ  Nam" |

### ğŸ“Œ Context Window lÃ  gÃ¬?

```
Context Window = Sá»‘ token tá»‘i Ä‘a LLM cÃ³ thá»ƒ "nhÃ¬n tháº¥y" cÃ¹ng lÃºc

GPT-3.5:    4,096 tokens   (~3,000 tá»«)
GPT-4:      128,000 tokens (~96,000 tá»«)
Claude 3:   200,000 tokens (~150,000 tá»«)
Gemini 1.5: 1,000,000 tokens (~750,000 tá»«)
```

### ğŸ“Œ VÃ­ dá»¥ thá»±c táº¿

```
Náº¿u báº¡n paste 1 cuá»‘n sÃ¡ch 100,000 tá»« vÃ o ChatGPT:
- GPT-3.5: Chá»‰ "tháº¥y" 3% Ä‘áº§u tiÃªn
- GPT-4: "Tháº¥y" gáº§n háº¿t
- Claude 3: "Tháº¥y" toÃ n bá»™
```

---

## 6.3 KNOWLEDGE OF SELF (Tá»± nháº­n thá»©c)

### ğŸ“Œ Keyword: Self-Knowledge, System Prompt

**LLM cÃ³ biáº¿t mÃ¬nh lÃ  ai khÃ´ng?**

KHÃ”NG thá»±c sá»±. Khi há»i "Báº¡n lÃ  ai?":
- LLM Ä‘oÃ¡n dá»±a trÃªn pattern trong training data
- CÃ³ thá»ƒ tráº£ lá»i sai (vÃ­ dá»¥: model cá»§a cÃ´ng ty A nÃ³i mÃ¬nh lÃ  cá»§a cÃ´ng ty B)

### ğŸ“Œ CÃ¡ch cÃ¡c cÃ´ng ty xá»­ lÃ½

```
System Prompt (Ä‘Æ°á»£c thÃªm vÃ o Ä‘áº§u má»—i conversation):

"You are Claude, made by Anthropic. 
Your knowledge cutoff is April 2024.
You should be helpful, harmless, and honest..."

â†’ "Hardcode" thÃ´ng tin vá» báº£n thÃ¢n vÃ o prompt
```

---

## 6.4 MODELS NEED TOKENS TO THINK

### ğŸ“Œ Keyword: Thinking Tokens, Step-by-Step, Chain-of-Thought

**PhÃ¡t hiá»‡n quan trá»ng:**
LLM cáº§n "khÃ´ng gian" (tokens) Ä‘á»ƒ suy nghÄ©. KhÃ´ng thá»ƒ tÃ­nh toÃ¡n phá»©c táº¡p trong 1 token.

### ğŸ“Œ VÃ­ dá»¥ minh há»a

```
CÃCH 1 - KhÃ´ng cho "khÃ´ng gian suy nghÄ©":
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: 17 x 24 = ?
LLM:  408  â† SAI (Ä‘Ã¡p Ã¡n Ä‘Ãºng lÃ  408... Ã  Ä‘Ãºng rá»“i, 
                   nhÆ°ng vá»›i sá»‘ phá»©c táº¡p hÆ¡n sáº½ sai)

User: 127 x 389 = ?  
LLM:  49,303  â† SAI (Ä‘Ã¡p Ã¡n Ä‘Ãºng: 49,403)


CÃCH 2 - Cho "khÃ´ng gian suy nghÄ©":
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: 127 x 389 = ? HÃ£y tÃ­nh tá»«ng bÆ°á»›c.

LLM:  Äá»ƒ tÃ­nh 127 x 389:
      
      127 x 300 = 38,100
      127 x 80 = 10,160
      127 x 9 = 1,143
      
      38,100 + 10,160 + 1,143 = 49,403
      
      ÄÃ¡p Ã¡n: 49,403  â† ÄÃšNG
```

### ğŸ“Œ BÃ i há»c thá»±c tiá»…n

```
âŒ Prompt kÃ©m:
"Cho tÃ´i káº¿t luáº­n cuá»‘i cÃ¹ng vá» váº¥n Ä‘á» X"

âœ… Prompt tá»‘t:
"PhÃ¢n tÃ­ch váº¥n Ä‘á» X tá»«ng bÆ°á»›c, sau Ä‘Ã³ Ä‘Æ°a ra káº¿t luáº­n"
```

---

## 6.5 JAGGED INTELLIGENCE (TrÃ­ thÃ´ng minh khÃ´ng Ä‘á»u)

### ğŸ“Œ Keyword: Jagged Intelligence, Uneven Capabilities

**LLM giá»i má»™t sá»‘ thá»©, dá»Ÿ má»™t sá»‘ thá»© khÃ¡c - khÃ´ng Ä‘á»u.**

### ğŸ“Œ VÃ­ dá»¥

```
LLM cÃ³ thá»ƒ:
âœ… Viáº¿t code phá»©c táº¡p
âœ… Giáº£i thÃ­ch váº­t lÃ½ lÆ°á»£ng tá»­
âœ… Viáº¿t thÆ¡, truyá»‡n ngáº¯n
âœ… Dá»‹ch 50 ngÃ´n ngá»¯

NhÆ°ng cÃ³ thá»ƒ fail:
âŒ Äáº¿m sá»‘ chá»¯ "r" trong "strawberry"
âŒ Giáº£i sudoku
âŒ Sáº¯p xáº¿p 10 sá»‘ theo thá»© tá»±
âŒ Nhá»› chÃ­nh xÃ¡c sá»‘ Ä‘iá»‡n thoáº¡i vá»«a Ä‘Æ°á»£c cho
```

### ğŸ“Œ Táº¡i sao?

```
LLM xá»­ lÃ½ á»Ÿ má»©c TOKEN, khÃ´ng pháº£i má»©c CHá»® CÃI.

"strawberry" cÃ³ thá»ƒ lÃ  1-2 tokens
â†’ LLM khÃ´ng "tháº¥y" tá»«ng chá»¯ cÃ¡i riÃªng biá»‡t
â†’ Äáº¿m chá»¯ cÃ¡i = task khÃ³
```

---

## 6.6 TOOL USE (Sá»­ dá»¥ng cÃ´ng cá»¥)

### ğŸ“Œ Keyword: Tool Use, Function Calling, Plugins

**LLM cÃ³ thá»ƒ há»c cÃ¡ch gá»i tools bÃªn ngoÃ i:**

```
Tools phá»• biáº¿n:
- Web Search: TÃ¬m kiáº¿m thÃ´ng tin má»›i
- Calculator: TÃ­nh toÃ¡n chÃ­nh xÃ¡c
- Code Interpreter: Cháº¡y code Python
- Image Generator: Táº¡o hÃ¬nh áº£nh
- File Reader: Äá»c file upload
```

### ğŸ“Œ VÃ­ dá»¥ Tool Use

```
User: GiÃ¡ Bitcoin hÃ´m nay lÃ  bao nhiÃªu?

LLM (trong Ä‘áº§u): ÄÃ¢y lÃ  thÃ´ng tin real-time, 
                  tÃ´i cáº§n dÃ¹ng web search.

LLM â†’ [Gá»i web_search("bitcoin price today")]
Web â†’ [Tráº£ vá»: $67,234.56]

LLM: GiÃ¡ Bitcoin hiá»‡n táº¡i lÃ  $67,234.56.
```

---

# 7. CÃCH Sá»¬ Dá»¤NG LLM HIá»†U QUáº¢

## 7.1 PROMPTING TIPS

### ğŸ“Œ Tip 1: Cho context Ä‘áº§y Ä‘á»§

```
âŒ KÃ©m: "Sá»­a code nÃ y"

âœ… Tá»‘t: "TÃ´i Ä‘ang viáº¿t Python script Ä‘á»ƒ xá»­ lÃ½ file CSV.
        Code dÆ°á»›i Ä‘Ã¢y bá»‹ lá»—i khi file cÃ³ dÃ²ng trá»‘ng.
        HÃ£y tÃ¬m vÃ  sá»­a lá»—i:
        [code]"
```

### ğŸ“Œ Tip 2: YÃªu cáº§u suy nghÄ© tá»«ng bÆ°á»›c

```
âŒ KÃ©m: "Cho tÃ´i Ä‘Ã¡p Ã¡n"

âœ… Tá»‘t: "HÃ£y phÃ¢n tÃ­ch tá»«ng bÆ°á»›c, sau Ä‘Ã³ Ä‘Æ°a ra Ä‘Ã¡p Ã¡n"
```

### ğŸ“Œ Tip 3: Cho vÃ­ dá»¥ (Few-shot learning)

```
âŒ KÃ©m: "PhÃ¢n loáº¡i sentiment cá»§a cÃ¢u sau"

âœ… Tá»‘t: "PhÃ¢n loáº¡i sentiment cá»§a cÃ¢u. VÃ­ dá»¥:
        - 'Sáº£n pháº©m tuyá»‡t vá»i!' â†’ Positive
        - 'Dá»‹ch vá»¥ tá»‡ quÃ¡' â†’ Negative
        - 'HÃ ng bÃ¬nh thÆ°á»ng' â†’ Neutral
        
        PhÃ¢n loáº¡i cÃ¢u sau: 'Giao hÃ ng nhanh, Ä‘Ã³ng gÃ³i cáº©n tháº­n'"
```

### ğŸ“Œ Tip 4: Specify format output

```
âŒ KÃ©m: "Cho tÃ´i thÃ´ng tin vá» 5 cÃ´ng ty tech lá»›n nháº¥t"

âœ… Tá»‘t: "Liá»‡t kÃª 5 cÃ´ng ty tech lá»›n nháº¥t theo market cap.
        Format:
        1. [TÃªn cÃ´ng ty] - [Market cap] - [NgÃ nh chÃ­nh]"
```

### ğŸ“Œ Tip 5: Cho phÃ©p nÃ³i "khÃ´ng biáº¿t"

```
âœ… ThÃªm vÃ o prompt:
"Náº¿u báº¡n khÃ´ng cháº¯c cháº¯n vá» thÃ´ng tin nÃ o, 
 hÃ£y nÃ³i rÃµ lÃ  khÃ´ng cháº¯c cháº¯n thay vÃ¬ Ä‘oÃ¡n."
```

---

## 7.2 KHI NÃ€O NÃŠN/KHÃ”NG NÃŠN TIN LLM

### ğŸ“Œ NÃªn tin khi:

| TÃ¬nh huá»‘ng | LÃ½ do |
|------------|-------|
| Giáº£i thÃ­ch concept | LLM giá»i tá»•ng há»£p vÃ  giáº£i thÃ­ch |
| Viáº¿t code cÆ¡ báº£n | ÄÃ£ tháº¥y nhiá»u code trong training |
| Brainstorming Ã½ tÆ°á»Ÿng | SÃ¡ng táº¡o, khÃ´ng cáº§n chÃ­nh xÃ¡c |
| Dá»‹ch thuáº­t | Training trÃªn nhiá»u ngÃ´n ngá»¯ |
| Viáº¿t/edit vÄƒn báº£n | Core strength cá»§a LLM |

### ğŸ“Œ Cáº©n tháº­n khi:

| TÃ¬nh huá»‘ng | LÃ½ do |
|------------|-------|
| Sá»‘ liá»‡u cá»¥ thá»ƒ | CÃ³ thá»ƒ hallucinate |
| ThÃ´ng tin má»›i (sau cutoff) | Knowledge bá»‹ outdated |
| TÃ­nh toÃ¡n phá»©c táº¡p | NÃªn dÃ¹ng calculator tool |
| TrÃ­ch dáº«n nguá»“n | CÃ³ thá»ƒ bá»‹a link, tÃªn sÃ¡ch |
| Quyáº¿t Ä‘á»‹nh quan trá»ng | LuÃ´n double-check |

---

## 7.3 NÆ I TÃŒM VÃ€ DÃ™NG LLM

### ğŸ“Œ Model Ä‘Ã³ng (Proprietary)

| Model | Website | Äáº·c Ä‘iá»ƒm |
|-------|---------|----------|
| ChatGPT | chat.openai.com | Phá»• biáº¿n nháº¥t |
| Claude | claude.ai | An toÃ n, context dÃ i |
| Gemini | gemini.google.com | TÃ­ch há»£p Google |

### ğŸ“Œ Model má»Ÿ (Open-weights)

| Model | NÆ¡i dÃ¹ng | Äáº·c Ä‘iá»ƒm |
|-------|----------|----------|
| Llama 3 | together.ai, huggingface | Meta, free |
| DeepSeek | chat.deepseek.com | Trung Quá»‘c, ráº» |
| Mistral | mistral.ai | PhÃ¡p, nháº¹ |

### ğŸ“Œ Cháº¡y local

| Tool | Äáº·c Ä‘iá»ƒm |
|------|----------|
| LM Studio | GUI Ä‘áº¹p, dá»… dÃ¹ng |
| Ollama | Command line, nháº¹ |
| llama.cpp | Tá»‘i Æ°u cho CPU |

---

# 8. Tá»ª ÄIá»‚N KEYWORDS QUAN TRá»ŒNG

## A-C

| Keyword | NghÄ©a tiáº¿ng Viá»‡t | Giáº£i thÃ­ch ngáº¯n |
|---------|-----------------|-----------------|
| **Attention** | CÆ¡ cháº¿ chÃº Ã½ | CÃ¡ch Transformer "nhÃ¬n" cÃ¡c pháº§n khÃ¡c nhau cá»§a input |
| **Backpropagation** | Lan truyá»n ngÆ°á»£c | Thuáº­t toÃ¡n Ä‘iá»u chá»‰nh parameters dá»±a trÃªn error |
| **Base Model** | MÃ´ hÃ¬nh gá»‘c | Model chá»‰ train pre-training, chÆ°a fine-tune |
| **BPE** | Byte Pair Encoding | Thuáº­t toÃ¡n tokenization phá»• biáº¿n |
| **Chain-of-Thought** | Chuá»—i suy nghÄ© | Ká»¹ thuáº­t Ä‘á»ƒ LLM "suy nghÄ©" tá»«ng bÆ°á»›c |
| **ChatML** | Chat Markup Language | Format chuáº©n cho conversation data |
| **Common Crawl** | - | Dataset thu tháº­p tá»« web |
| **Context Window** | Cá»­a sá»• ngá»¯ cáº£nh | Sá»‘ token tá»‘i Ä‘a LLM cÃ³ thá»ƒ xá»­ lÃ½ |
| **Cutoff Date** | NgÃ y cáº¯t | Thá»i Ä‘iá»ƒm training data káº¿t thÃºc |

## D-H

| Keyword | NghÄ©a tiáº¿ng Viá»‡t | Giáº£i thÃ­ch ngáº¯n |
|---------|-----------------|-----------------|
| **Deduplication** | Loáº¡i bá» trÃ¹ng láº·p | BÆ°á»›c lá»c data trÃ¹ng |
| **Fine-tuning** | Tinh chá»‰nh | Train thÃªm model cÃ³ sáºµn trÃªn data má»›i |
| **FineWeb** | - | Dataset Ä‘Ã£ lá»c sáº¡ch 44TB |
| **Function Calling** | Gá»i hÃ m | Kháº£ nÄƒng LLM gá»i tools bÃªn ngoÃ i |
| **Hallucination** | áº¢o giÃ¡c | LLM bá»‹a thÃ´ng tin |
| **Human Labelers** | NgÆ°á»i gÃ¡n nhÃ£n | NgÆ°á»i táº¡o training data cháº¥t lÆ°á»£ng cao |

## I-P

| Keyword | NghÄ©a tiáº¿ng Viá»‡t | Giáº£i thÃ­ch ngáº¯n |
|---------|-----------------|-----------------|
| **Inference** | Suy luáº­n | QuÃ¡ trÃ¬nh model táº¡o output tá»« input |
| **Instruct Model** | Model hÆ°á»›ng dáº«n | Model Ä‘Ã£ fine-tune Ä‘á»ƒ lÃ m theo lá»‡nh |
| **Jagged Intelligence** | TrÃ­ thÃ´ng minh khÃ´ng Ä‘á»u | LLM giá»i/dá»Ÿ khÃ´ng Ä‘á»u cÃ¡c task |
| **LLM** | Large Language Model | MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n |
| **Next Token Prediction** | Dá»± Ä‘oÃ¡n token tiáº¿p | Nhiá»‡m vá»¥ cÆ¡ báº£n cá»§a LLM |
| **Parameters** | Tham sá»‘ | CÃ¡c "nÃºt váº·n" trong neural network |
| **PII** | Personal Identifiable Info | ThÃ´ng tin cÃ¡ nhÃ¢n cáº§n loáº¡i bá» |
| **Pre-training** | Huáº¥n luyá»‡n trÆ°á»›c | Giai Ä‘oáº¡n train Ä‘áº§u tiÃªn trÃªn web data |
| **Prompt** | CÃ¢u lá»‡nh | Input báº¡n Ä‘Æ°a cho LLM |

## R-Z

| Keyword | NghÄ©a tiáº¿ng Viá»‡t | Giáº£i thÃ­ch ngáº¯n |
|---------|-----------------|-----------------|
| **RAG** | Retrieval Augmented Generation | Ká»¹ thuáº­t cung cáº¥p context cho LLM |
| **Reasoning Model** | Model suy luáº­n | LLM cÃ³ kháº£ nÄƒng "suy nghÄ©" (O1, DeepSeek-R1) |
| **Reward Model** | Model pháº§n thÆ°á»Ÿng | Model cho Ä‘iá»ƒm output trong RLHF |
| **RLHF** | RL from Human Feedback | PhÆ°Æ¡ng phÃ¡p dÃ¹ng feedback ngÆ°á»i Ä‘á»ƒ cáº£i thiá»‡n |
| **SFT** | Supervised Fine-tuning | Fine-tuning cÃ³ giÃ¡m sÃ¡t |
| **System Prompt** | Prompt há»‡ thá»‘ng | Instructions áº©n Ä‘áº§u má»—i conversation |
| **Token** | - | ÄÆ¡n vá»‹ nhá» nháº¥t LLM xá»­ lÃ½ |
| **Tokenization** | MÃ£ hÃ³a token | Chuyá»ƒn text thÃ nh tokens |
| **Transformer** | - | Kiáº¿n trÃºc neural network cho LLM |
| **Vocabulary** | Tá»« vá»±ng | Táº­p há»£p táº¥t cáº£ tokens cÃ³ thá»ƒ |
| **Working Memory** | Bá»™ nhá»› lÃ m viá»‡c | Context hiá»‡n táº¡i trong conversation |

---

# ğŸ“š TÃ€I NGUYÃŠN THAM KHáº¢O

## Video gá»‘c
- [Deep Dive into LLMs like ChatGPT - Andrej Karpathy](https://youtube.com/watch?v=7xTGNNLPyMI)

## Theo dÃµi cáº­p nháº­t AI
- [LMArena Leaderboard](https://lmarena.ai/) - Báº£ng xáº¿p háº¡ng LLM
- [AI News Newsletter](https://buttondown.email/ainews) - Newsletter AI
- Follow Andrej Karpathy trÃªn X/Twitter

## Há»c thÃªm
- Andrej Karpathy YouTube Channel
- [llm.c](https://github.com/karpathy/llm.c) - Train GPT-2 vá»›i C
- [nanoGPT](https://github.com/karpathy/nanoGPT) - GPT implementation Ä‘Æ¡n giáº£n

---

*TÃ i liá»‡u Ä‘Æ°á»£c táº¡o Ä‘á»ƒ giáº£i thÃ­ch video "Deep Dive into LLMs like ChatGPT" cá»§a Andrej Karpathy cho ngÆ°á»i má»›i báº¯t Ä‘áº§u.*
